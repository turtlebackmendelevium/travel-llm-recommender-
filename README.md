# ğŸ§  Travel LLM Recommender

A smart, modular recommendation system that explores and compares **three major techniques** for travel recommendations using **Large Language Models (LLMs)**:

- âœ… **Prompt Engineering**
- âœ… **Fine-Tuning a T5 Transformer**
- âš™ï¸ **Retrieval-Augmented Generation (RAG)** *(in progress)*

> ğŸš§ This is a **work-in-progress project** intentionally structured to reflect real-world iterative development.  
> ğŸ’¼ This README clearly outlines whatâ€™s complete, whatâ€™s underway, and how each part reflects applied AI/ML and data engineering skills.

---

## ğŸ” Project Goal

Build a smart travel assistant that recommends destinations using natural language queries and integrates:

- LLMs via Hugging Face Transformers  
- Custom dataset curation from TripAdvisor and European tourism sources  
- Vector search using FAISS  
- Text embedding using Sentence Transformers  
- Comparison of different LLM-based strategies for recommendation

---

## âœ… Completed Components

### 1. ğŸ”§ Prompt Engineering

- Built structured, optimized prompts to get high-quality travel suggestions from LLM APIs (OpenAI, DeepSeek)
- Demonstrated impact of temperature, instruction clarity, and token limits
- Designed prompt templates for reproducibility and easy testing

### 2. ğŸ§ª Fine-Tuning a T5 Model

- Cleaned and formatted 500-entry dataset into Hugging Faceâ€“compatible JSONL format
- Fine-tuned a **T5 model** using the Hugging Face `Trainer` API
- Used **PyTorch** backend with GPU acceleration
- Implemented training checkpoints and saved the final model under:
- Avoided overfitting by controlling input-output similarity in training data

### 3. ğŸ“ Data Preprocessing & EDA

- Combined and cleaned datasets from:
- TripAdvisor hotel reviews
- 2023 traveler reviews
- European tourist destinations
- Handled encoding issues (`utf-8`)
- Used `pandas` and `langchain_community.document_loaders.CSVLoader`
- Explored destination clusters and patterns for grounding LLM output

---

## ğŸš§ In Progress

### 4. ğŸ”„ Retrieval-Augmented Generation (RAG)

- Setup includes:
- `RecursiveCharacterTextSplitter` for document chunking
- `HuggingFaceEmbeddings` with `all-MiniLM-L6-v2`
- `FAISS` for semantic vector indexing
- Status:
- Document splitting initiated on multi-thousand row CSV corpus
- FAISS index build is currently underway (expected due to large corpus size)
- Next Steps:
- Save index and test LangChain-based query inference
- Validate RAG vs fine-tuned and prompt-based results

---

## ğŸ› ï¸ Tech Stack

| Tool | Purpose |
|------|---------|
| ğŸ Python | Core programming |
| ğŸ§  Hugging Face Transformers | LLMs, tokenizers, fine-tuning |
| ğŸ”— LangChain | RAG and embedding pipelines |
| ğŸ§® FAISS | Vector similarity search |
| ğŸ§¬ Sentence Transformers | `all-MiniLM-L6-v2` for embedding |
| âš™ï¸ PyTorch | Fine-tuning backend |
| ğŸ“Š pandas / NumPy | Data preprocessing |
| ğŸ“ JSONL / CSV | Dataset formats |
| ğŸ’» VS Code | Dev environment |
| ğŸ§ª Jupyter | Local EDA experiments |

---

### ğŸ“ Folder Structure

```bash
travel-llm-recommender/
â”œâ”€â”€ finetune/
â”‚   â”œâ”€â”€ prepare_data.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ dataset.jsonl
â”‚   â””â”€â”€ output/
â”‚       â””â”€â”€ final-model/
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ build_index.py          # Indexing logic (FAISS)
â”‚   â”œâ”€â”€ querry_rag.py           # Inference using vector store
â”œâ”€â”€ datasets_raw/               # Raw CSV datasets
â””â”€â”€ README.md                   # Project documentation
```
---

## ğŸ“¦ Download Required Datasets

To keep the repository lightweight, large data folders are hosted externally on Google Drive.  
Please manually download and extract the following folders:

- [`datasets_raw/`](https://drive.google.com/drive/folders/1VKZX22fWZjsI6xKxy8oFEc2UxH3_V5Nz?usp=sharing) â€“ Raw CSVs and travel data


### How to Set Up:

1. Click each link above and choose **â€œDownloadâ€** from the Google Drive interface.
2. Google will deliver a `.zip`; unzip it on your machine.

---

## âœï¸ What I Learned

- Fine-tuning transformer models with domain-specific data
- Vector search pipelines and hybrid LLM architectures
- Real-world dataset issues (formatting, encoding, redundancy)
- Comparison of prompt engineering vs. learned representations
- Integrating tools across Hugging Face, LangChain, FAISS, and PyTorch

---

## ğŸ“Œ Why This Project Matters

This project demonstrates:

- ğŸ” Depth in working with LLMs at various levels (prompting, fine-tuning, RAG)
- ğŸ§± Modular design that mirrors real-world system building
- ğŸ“š Clear separation between exploration, modeling, and retrieval
- ğŸ§© Realistic challenges in large-scale unstructured data handling

> ğŸ§  Even though RAG is still in progress, this project shows full-cycle model development, thoughtful architecture, and the ability to build and debug deep AI systems from scratch.

---

---

## ğŸ”œ Next Steps

- Complete FAISS indexing and integrate RAG inference  
- Benchmark all 3 approaches (prompting, fine-tune, RAG)  
- Build Streamlit UI for public demo  
- Dockerize the pipeline for reproducible deployment

---


ğŸ¤ Connect & Collaborate
This project reflects my commitment to building real-world AI systems that integrate LLMs, vector search, and modern machine learning workflows.
If you're working on anything at the intersection of AI, data, or productâ€”and want to trade ideas or collaborateâ€”I'm always open to meaningful conversations and innovative work.

ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/mohammad-abbasi-393254263/)

ğŸ§  [GitHub](https://github.com/turtlebackmendelevium)

ğŸ“¨ [Email](ayaan.abbasi01@outlook.com)



---

